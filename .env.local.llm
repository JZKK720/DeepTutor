# =============================================================================
# Local LLM Configuration (Ollama + LM Studio)
# Copy these values to your .env file
# =============================================================================

# =============================================================================
# LLM Configuration (Using Ollama)
# =============================================================================
LLM_BINDING=ollama
LLM_MODEL=llama3.2:latest  # Change to your model name
LLM_API_KEY=ollama  # Ollama doesn't need a real key
LLM_HOST=http://host.docker.internal:11434

# =============================================================================
# Embedding Configuration (Using Ollama)
# =============================================================================
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=nomic-embed-text:latest  # Change to your embedding model
EMBEDDING_API_KEY=ollama
EMBEDDING_HOST=http://host.docker.internal:11434
EMBEDDING_DIMENSION=768  # Depends on your model (nomic-embed-text = 768)

# =============================================================================
# Alternative: Using LM Studio
# =============================================================================
# LLM_BINDING=lm_studio
# LLM_MODEL=your-model-name
# LLM_API_KEY=lm-studio
# LLM_HOST=http://host.docker.internal:14321/v1

# EMBEDDING_BINDING=lm_studio
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_API_KEY=lm-studio
# EMBEDDING_HOST=http://host.docker.internal:14321/v1
# EMBEDDING_DIMENSION=1536

# =============================================================================
# Local LLM Provider Ports (for Docker host.docker.internal access)
# =============================================================================
OLLAMA_PORT=11434
LMSTUDIO_PORT=14321
